{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"AI Human SDK Manual for Windows Version 1.3.0 Latest Updated 2022.10.25 This document is a step-by-step manual describing the Sample Solution based on DeepBrain AI's AI Human SDK and WPF (.NET 5). Documentation for distribution \u00a9 2021 DeepBrain AI. All rights reserved","title":"AI Human SDK Manual for Windows"},{"location":"#ai-human-sdk-manual-for-windows","text":"Version 1.3.0 Latest Updated 2022.10.25 This document is a step-by-step manual describing the Sample Solution based on DeepBrain AI's AI Human SDK and WPF (.NET 5). Documentation for distribution \u00a9 2021 DeepBrain AI. All rights reserved","title":"AI Human SDK Manual for Windows"},{"location":"AIHumanQuickStart/","text":"AIHuman Quick Start In this chapter, we will quickly set up AIPlayer with the default AI and learn about AI speaking process. When setting up AIPlayer for the first time, it may take several minutes to load depending on the network condition. 1. The SDK authentication process is performed in the partial class App : System.Windows.Application. Create a project in SDK Website , enter App Id of Windows and click confirm. Then User Key will be issued. Implement the AuthStart function: input appId, userKey, uuid, and platform information. - appId is a unique Id of the project, and is generally created like \"com.example.project.appname\". - userkey can be obtained by creating a project on the AIHuman website and registering the appId. - uuid refers to the unique ID of the device where the application is installed. It is created by calling Guid.NewGuid(), and saved and reused after initial creation so that it does not change each time it is called. - platform argument uses \"wnds\", which means windows. 2. Create AIPlayer through CreateQuickStartViewModel using INavigationService. 1. Create a QuickStartView and a QuickStartViewModel that will contain AIPlayer. 2. Create a Layout Component (parent layout) to which AIPlayer will be added at the location where you want to show AIPlayer of QuickStartView. 3. Binding AIPlayer Object of QuickStartViewModel to QuickStartView. 3. In QuickStartViewModel, it is possible to receive callback of AIState by IAIPlayerCallback. Implement the command for speaking and default AI operation. private void Speak_Command(object args) { if (CanSpeak && string.IsNullOrEmpty(InputText) == false) { _aiPlayer.Send(new[] { InputText }); SpeechList.Add($\"{_aiPlayerContext.AIName}: {InputText}\"); InputText = string.Empty; CanSpeak = false; } } There are many omitted parts in the above explanation. Please refer to App.xaml, QuickStartView.xaml, and QuickStartViewModel.cs files by opening the Solution file of the given Sample.","title":"AIHuman Quick Start"},{"location":"AIHumanQuickStart/#aihuman-quick-start","text":"In this chapter, we will quickly set up AIPlayer with the default AI and learn about AI speaking process. When setting up AIPlayer for the first time, it may take several minutes to load depending on the network condition. 1. The SDK authentication process is performed in the partial class App : System.Windows.Application. Create a project in SDK Website , enter App Id of Windows and click confirm. Then User Key will be issued. Implement the AuthStart function: input appId, userKey, uuid, and platform information. - appId is a unique Id of the project, and is generally created like \"com.example.project.appname\". - userkey can be obtained by creating a project on the AIHuman website and registering the appId. - uuid refers to the unique ID of the device where the application is installed. It is created by calling Guid.NewGuid(), and saved and reused after initial creation so that it does not change each time it is called. - platform argument uses \"wnds\", which means windows. 2. Create AIPlayer through CreateQuickStartViewModel using INavigationService. 1. Create a QuickStartView and a QuickStartViewModel that will contain AIPlayer. 2. Create a Layout Component (parent layout) to which AIPlayer will be added at the location where you want to show AIPlayer of QuickStartView. 3. Binding AIPlayer Object of QuickStartViewModel to QuickStartView. 3. In QuickStartViewModel, it is possible to receive callback of AIState by IAIPlayerCallback. Implement the command for speaking and default AI operation. private void Speak_Command(object args) { if (CanSpeak && string.IsNullOrEmpty(InputText) == false) { _aiPlayer.Send(new[] { InputText }); SpeechList.Add($\"{_aiPlayerContext.AIName}: {InputText}\"); InputText = string.Empty; CanSpeak = false; } } There are many omitted parts in the above explanation. Please refer to App.xaml, QuickStartView.xaml, and QuickStartViewModel.cs files by opening the Solution file of the given Sample.","title":"AIHuman Quick Start"},{"location":"AIPlayerDescription/","text":"AIPlayer Description In this chapter, we will learn how to set up and use the AIPlayer object that can perform specific actions by actually displaying AI Human. AIPlayer consists of UserControl type View and ViewModel that controls related routines. For more details, please refer to the contents below and the main class API manual. Dev Tips! The concepts of MVVM, Dependency Injection, and Data Binding are applied to AI Human SDK and WPF Sample. When developing a custom app by using the SDK, create an AIHuman.Media.AIPlayer object. When composing a screen, obtain a View object by using the GetObject() function of AIPlayer and place it. It is recommended to use Data Binding for properties such as margin, size, and speed in AIPlayer. For ViewModel related to SDK in Custom App, it is recommended to inherit and implement AIHuman.Common.Base.ViewModelBase, AIHuman.Interface.IAIPlayerCallback. It is recommended to call the Dispose() function of the object before destroying or terminating the AIPlayer. (AIPlayerViewModel is a Dispoable object.) 1. AIPlayer Set up Full setup process (4 steps) Step 1: Add AIPlayer's parent layout to the page you want to use Step 2: Prepare AppID, authentication key, uuid, and platform that will be used in the AuthStart function Step 3: Get the AI to use by implementing the AuthStart response Step 4: Initialize AIPlayer to the desired AI You can create AIPlayer and get the View(UserControl) object through the GetObject() function. // practical use example private AIPlayer _aiPlayer; // AIPlayer object to be used in cs public AIPlayerView AIPlayerObject // View (UserControl) of AIPlayer to be used in xaml { get => _aiPlayer.GetObject(); private set => OnPropertyChanged(nameof(AIPlayerObject)); } Step 1. Organize your layout Configure the View to use AI Human in the XAML file. Create a ContentControl to place the AI and bind the AIPlayer. In the CS file, the property of the actual binding object is defined. public AIPlayerView AIPlayerObject { get => _aiPlayer.GetObject(); private set => OnPropertyChanged(nameof(AIPlayerObject)); } ... <Grid> <Grid.ColumnDefinitions> <ColumnDefinition/> <ColumnDefinition/> </Grid.ColumnDefinitions> <ContentControl Margin=\"0\" Grid.Column=\"0\" Content=\"{Binding Path=AIPlayerObject}\" /> </Grid> ... Step 2. Prepare authentication-related information The AuthStart function requires 4 parameters. These four are AppID, UserKey, uuid, and Platform information. UserKey is a unique string generated by DeepBrain AI and should never be disclosed. If you call the API using this authentication key, you will receive available Default AI data and a token to be used in the future. If token refresh is required because the token expired, it can be refreshed by calling AuthStart() again. Step 3. Implement AuthStart and Get the AI list If you have all the necessary AuthStart function parameters mentioned in step 2, you are ready for authentication. Input these parameters inti AuthStart AIHuam.AIAPI.Instance and implement a callback function. If authentication is successful, AI list will be returned. If you do not have permission to any of the AIs, aiList returns null. AIHuman.Core.AIAPI.Instance.AuthStart(\"appId\", \"userKey\", \"UUID\", \"wnds\" , (aiLIst, error) => { string message = string.Empty; if(string.IsNullOrEmpty(error) && aiLIst != null) { string jsonStr = aiLIst.Root.ToString(); AIHuman.Core.AIAPI.AIList list = Newtonsoft.Json.JsonConvert.DeserializeObject<AIHuman.Core.AIAPI.AIList>(jsonStr); message = string.Format(\"Auth Complete, Avaliable Count : {0}\", list.ai.Length); /* e.g.) \"succeed\":true, \"ai\":[{\"aiName\":\"vida\",\"aiDisplayName\":\"Vida\",\"language\":\"en\"}, {\"aiName\":\"bret\",\"aiDisplayName\":\"Bret\",\"language\":\"en\"}, {\"aiName\":\"danny\",\"aiDisplayName\":\"Danny\",\"language\":\"en\"}, {\"aiName\":\"kang\",\"aiDisplayName\":\"Kang\",\"language\":\"ko\"}] */ } else { message = string.Format(\"Auth Error : {0}\", error); } AIHuman.Util.Log.LogWrite(message); } ); Step 4. Initialize AIPlayer to the desired AI After authenticating and checking the list of available AIs, it is necessary to initialize the AI first in order to actually use a specific AI. To initialize it, create AIPlayer with the desired AIName as shown below. If there is an existing AIPlayer, delete it and create a new one. Once you initialize, AIPlayer downloads necessary resources based on initial settings and becomes active. You can also receive AI status through callback function registered in the second argument of the AIPlayer constructor. ... public AIPlayer AIPlayerObject // View Binding object { ... } ... private void UpdateSelectedAI() { if (_aiPlayer != null) { _aiPlayer.Dispose(); _aiPlayer = null; } if (_speechList != null) { _speechList.Clear(); _speechList = null; } AIPlayer _aiPlayer = new(SelectedAI.AIName, this); AIPlayerObject = _aiPlayer.GetObject(); SpeechList = new ObservableCollection<string>(AIAPI.Instance.GetSampleTexts(SelectedAI.AIName)); SpeechList.Insert(0, Resource.DefaultSpeech); ... } 2. AIPlayer Resources and States Start loading resources When AIPlayer is created after authentication is completed, resource loading starts according to the input AIName , and the resource loading status is reported to the listener (IAIPlayerCallback) registered in the constructor. (Initially, it may take a few minutes for the resource to complete loading.) Monitoring player state through IAIPlayerCallback implementation The values for the parameter AIState.state in the listener method onAIStateChanged(AIStatePublisher.AIState state) are shown below. You can also implement loading progress with onAIPlayerResLoadingProgressed(int current, int total). AIState.RES_LOAD_STARTED : resource loading is started. AIState.RES_LOAD_COMPLETED : resource loading is completed. If there is any problem during this process, the onAIPlayerError() method is called. Typically, a response from the onAIPlayerError() may be notifying the expiration of the authentication token. An appropriate response is required depending on the situation. AIError.SDK_API_ERR : Notifies error in authentication process API. ##### e.g.) 1402 error (value token expired): Token refresh required -> Call AuthStart() method again // AI resource related status CallBack public void onAIStateChanged(AIState state) { ... if (state.state == AIState.RES_LOAD_STARTED) { message = \"AI Resource loading started.\"; ... } else if (state.state == AIState.RES_LOAD_COMPLETED) { message = \"AI Resource loading completed.\"; ... } ... } // AI resource loading progress CallBack public void onAIPlayerResLoadingProgressed(int current, int total) { float progress = ((float) current / (float) total) * 100; message = string.Format(\"AI Resource Loading... {0}%\", (int)progress); } // AI error CallBack public void onAIPlayerError(AIError error) { if (error.SDK_API_ERR == error.errorType) { Debug.LogError(string.Format(\"sdk_ai_Info error : {0}\", error.exInfo)); string errorDesc = error.exInfo; if (string.IsNullOrEmpty(errorDesc)) { JSONObject json = null; try { json = new JSONObject(errorDesc); } catch (JsonException ex) { Log.log(ex.Message); Debug.LogError(string.Format(\"Json Exception Error : {0}\", ex.Message)); } if (json != null && json.optInt(Constants.KEY_ERRORCODE, -1) == Constants.API_ERRORCODE_TOKEN_EXPIRED) { // refresh token } } } } 3. AIPlayer Basic Speaking Features Basic Speaking using AIClipSet and Monitor AI Speaking After AIPlayer resource is loaded, call Send method . To activate the function, in the sample below, select the sentence to speak through the drop-down menu and click the Play button on the right. In general, speech can be performed using pure text, but speech can also be performed using AIHuman.Common.Model.AIClipSet . Also, speech can be performed along with a specific gesture. For example, you could instruct the AI to say hello by waving his hand. This is called gesture speech. Details are explained in Gesture speech related parts . If the text to speak is too long, it may not be possible to synthesize the resources required for the utterance. There are some models that can synthesize long sentences. Although it varies from ai to ai, it is generally recommended that sentences be cut to an appropriate length in Korean, usually within 30 to 40 characters, and at a similar level in English. // using pure-text _aiPlayer.Send(new[] {\"this is sample sentence.\"}); // using AIClipSet AIClipSet clip = AIAPI.CreateClipSet(\"this is sample sentence.\"); _aiPlayer.Send(new[] {clip}); Speaking related Monitoring After the Send method is called, you can listen to the operation status feedback in the registered listener. This feedback is returned by calling the method (onAIStateChanged) of the listener(IAIPlayerCallback). onAIStateChanged sequentially returns the following AIState values. SPEAKING_PREPARE_STARTED SPEAKING_PREPARE_COMPLETED SPEAKING_STARTED SPEAKING_COMPLETED // Speaking related CallBack example public void onAIStateChanged(AIState state) { if (state.state == AIState.SPEAKING_PREPARE_STARTED) { _txtStatus.text = \"AI started preparation to speak.\"; } else if (state.state == AIState.SPEAKING_PREPARE_COMPLETED) { _txtStatus.text = \"AI finished preparation to speak.\"; } else if (state.state == AIState.SPEAKING_STARTED) { _txtStatus.text = \"AI started speaking.\"; } else if (state.state == AIState.SPEAKING_COMPLETED) { _txtStatus.text = \"AI finished speaking.\"; } } // AI error CallBack example public void onAIPlayerError(AIError error) { if (error.errorType == AIError.SOCKET_ERR) { _txtStatus.text = \"Socket Error: \" + error.exInfo; } else if (error.errorType == AIError.RES_LOAD_ERR) { _txtStatus.text = \"Resource Error: \" + error.exInfo); } else if (error.errorType == AIError.SPEAK_SEND_ERR) { _txtStatus.text = \"Speak Error: \" + error.exInfo); } } AIPlayer Speaking related Features The following are actions that can be performed while the AIPlayer is Speaking. Pause speaking : Pause speaking. // pause method _aiPlayer.Pause() Resume Speaking : Resume speaking. (resume from pause) // resume method _aiPlayer.Resume() Stop speaking : Stop speaking and reset all data. (cannot resume) // stop method _aiPlayer.StopSpeaking() 4. AIPlayer Advanced Speaking Features All functions other than speaking(mostly related to AI settings) of AIPlayer are described below. After the resource load required for AI operation is completed, some settings of AIPlayer can be adjusted. When the resource loading is completed (RES_LOAD_COMPLETED), the state changes such that actual operations can be performed(Idle). On right side of the panel, Voice, Gesture, Speed , etc. can be adjusted as shown below. Change AI Speech Rate : You can set the speech rate of AI. The possible value range is from 0.5 to 1.5. // set Property _aiPlayer.Speed = value; Gestures As briefly mentioned above, speech can also be performed using ClipSet . The ClipSet here refers to one action unit in a series of AI actions. There are three types of ClipSet: general speech that performs only speaking, speech with gesture, and gesture only. The Gesture can be used depending on whether the AI model supports Gestures , and the list of available gestures can be checked using the GetGestures function of AIPlayer. Even a model that does not support gestures can be operated using ClipSet. Clipset types are as follows. Clip Type CLIP_SPEECH: Clip only for speech without gestures CLIP_GESTURE: Gesture only Clip CLIP_SPEECH_GESTURE: Clip for speech with gestures In the sample screenshot below, an AI model named Jonathan is speaking while waving his hand with a \"hi\" gesture. using AIHuman.Common.Model; using AIHuman.Core; using AIHuman.Media; ... private ObservableCollection<AIGesture> _gestures; ... _gestures = _aiPlayer.GetGestures(); ... AIGesture gesture = _gestures[index]; AIClipSet clip = AIAPI.CreateClipSet(\"nice to meet you.\", gesture.Name); _aiPlayer.Send(new[] {clip}); Monitoring callbacks of gesture actions IAIPlayerCallback.OnAIStateChanged(AIState) is called in the same way as the speech actions. The state value of AIState is called as follows to know the state. However, since AIState.GetAIMsg().Clip.Type, GestureName, and SpeechText are known here, it is possible to know whether it is a gesture action or just a speech action. SPEAKING_PREPARE_STARTED SPEAKING_PREPARE_COMPLETED SPEAKING_STARTED SPEAKING_COMPLETED Change the Voice or Language Some AIs can speak with other voices besides basic voices. It is also possible to speak other language than the basic voice's language if the supported voice's language is different from the basic language of AI. You can check the sample for a list of voices that are currently available to a AI. Set the custom voice using AIPlayer's method You can check which voice AI can use by the following method. CustomVoice has properties of id, name, language, and tag. ObservableCollection<CustomVoice> customVoices = _aiPlayer.GetCustomVoices(); If you know the id of the desired voice, you can find the desired voice using the following method. If there is none, return null. CustomVoice myVoice = _aiPlayer.FindCustomVoice(voiceId); Direct change to the desired voice on the aplayer is set as follows, and is set to the default voice when null is entered. Returns true when success. CustomVoice myVoice = _aiPlayer.GetCustomVoices()[2]; _aiPlayer.SetCustomVoice(myVoice); Set the custom voice using AIClipSet In addition to the method of using the setCustomVoice method to set a voice other than the basic voice, AIClipSet can be used to speak the desired voice as follows. CustomVoice myVoice = _aiPlayer.GetCustomVoices()[0]; AIClipSet aiClipSet = AIAPI.CreateClipSet(\"this is sample sentence.\", null, myVoice); _aiPlayer.Send(new[] {aiClipSet}); Preload Preload is used when you want to make the AI speak the next sentence without delay by loading sentences in advance. You could think of it as a caching process. Select a sentence and press the Preload button in the sample below to perform the corresponding action. // using pure-text _aiPlayer.Preload(new[] {\"sentence\"}); // using AIClipSet _aiPlayer.Preload(new[] {clip}); Preload related Monitoring AIPlayerCallback.onAIStateChanged(AIState) is called during the preload operation just like the speaking operation. The value of AIState is shown below. SPEAKING_PREPARE_PRELOAD_STARTED SPEAKING_PREPARE_PRELOAD_COMPLETED When the AI has several sentences to speak, it first processes the very first sentence. Once the returned state from onAIStateChanged is SPEAKING_STARTED, which is when the AI starts to speak the first sentence, the next sentence can be preloaded. If you play the next sentence after the state update to SPEAKING_PREPARE_PRELOAD_COMPLETED, there will be minimum delays between sentences. // AI Preload related CallBack public void onAIStateChanged(AIState aiState) { ... if (aiState.state == AIState.SPEAKING_PREPARE_PRELOAD_STARTED) { _txtStatus.text = \"AI started preparation to preload.\"; } else if (aiState.state == AIState.SPEAKING_PREPARE_PRELOAD_COMPLETED) { _txtStatus.text = \"AI finished preparation to preload.\"; } ... } Speak Multiple Sentences Consecutively You can give AIPlayer several sentences at once and make them speak sequentially. In the sample, multi-speaking is performed by selecting random sentences from sentences in the ComboBox. It can be one sentence or it can be several sentences. Press the Multi Speak button in the app below to perform the operation. // using pure-text _aiPlayer.Send(new[] {\"sentence1\", \"sentence2\"}); // using AIClipSet _aiPlayer.Send(new[] {clip1, clip2}); Multi Speak related Monitoring IAIPlayerCallback.onAIStateChanged(AIState) is called for each sentence. The possible AIState values are shown below. SPEAKING_PREPARE_STARTED SPEAKING_PREPARE_COMPLETED If you send several sentences, it automatically preloads if possible. In this case, you can see that the delay between utterances when the AI speaks is reduced. 5. Functionalities other than AI Speaking (mainly related to AI settings) After the resource is loaded, some settings of aiPlayer can be changed while the actual operation is on. In the sample project screen below, you can see that Scale, Margins , etc. can be adjusted. Change AI Size(Scale) : You can set the size(scale) of AI. The possible value range is from 0.5 to 1.5. // set Property _aiPlayer.Scale = value; Change AI Position(Margin) : You can change the position(margins) of AI. It can be adjusted based on the X-axis(Horizontal) and the Y-axis(Vertical). AIHuman.Common.Margin _aiMargin; _aiMargin.X = 64; _aiMargin.Y = 8; // set Property _aiPlayer.Margin = _aiMargin; 6. Error Index You can receive the error code and its details as a callback(onAIPlayerError) and take appropriate action. When an error occurs, the onAIPlayerError(AIError) callback function is called. AIError, the argument of this function, contains information about the error. AIError.errorType tells what kind of error has occurred, and you can find out the details of the error as JSON String through the getMessage() function. By using this message, you can take action when a specific error occurs. For example, code 1402 may mean Token expired, and in this case, call AuthStart() to refresh the token. Check the full error types here .","title":"AIPlayer Description"},{"location":"AIPlayerDescription/#aiplayer-description","text":"In this chapter, we will learn how to set up and use the AIPlayer object that can perform specific actions by actually displaying AI Human. AIPlayer consists of UserControl type View and ViewModel that controls related routines. For more details, please refer to the contents below and the main class API manual. Dev Tips! The concepts of MVVM, Dependency Injection, and Data Binding are applied to AI Human SDK and WPF Sample. When developing a custom app by using the SDK, create an AIHuman.Media.AIPlayer object. When composing a screen, obtain a View object by using the GetObject() function of AIPlayer and place it. It is recommended to use Data Binding for properties such as margin, size, and speed in AIPlayer. For ViewModel related to SDK in Custom App, it is recommended to inherit and implement AIHuman.Common.Base.ViewModelBase, AIHuman.Interface.IAIPlayerCallback. It is recommended to call the Dispose() function of the object before destroying or terminating the AIPlayer. (AIPlayerViewModel is a Dispoable object.)","title":"AIPlayer Description"},{"location":"AIPlayerDescription/#1-aiplayer-set-up","text":"Full setup process (4 steps) Step 1: Add AIPlayer's parent layout to the page you want to use Step 2: Prepare AppID, authentication key, uuid, and platform that will be used in the AuthStart function Step 3: Get the AI to use by implementing the AuthStart response Step 4: Initialize AIPlayer to the desired AI You can create AIPlayer and get the View(UserControl) object through the GetObject() function. // practical use example private AIPlayer _aiPlayer; // AIPlayer object to be used in cs public AIPlayerView AIPlayerObject // View (UserControl) of AIPlayer to be used in xaml { get => _aiPlayer.GetObject(); private set => OnPropertyChanged(nameof(AIPlayerObject)); }","title":"1. AIPlayer Set up"},{"location":"AIPlayerDescription/#step-1-organize-your-layout","text":"Configure the View to use AI Human in the XAML file. Create a ContentControl to place the AI and bind the AIPlayer. In the CS file, the property of the actual binding object is defined. public AIPlayerView AIPlayerObject { get => _aiPlayer.GetObject(); private set => OnPropertyChanged(nameof(AIPlayerObject)); } ... <Grid> <Grid.ColumnDefinitions> <ColumnDefinition/> <ColumnDefinition/> </Grid.ColumnDefinitions> <ContentControl Margin=\"0\" Grid.Column=\"0\" Content=\"{Binding Path=AIPlayerObject}\" /> </Grid> ...","title":"Step 1. Organize your layout"},{"location":"AIPlayerDescription/#step-2-prepare-authentication-related-information","text":"The AuthStart function requires 4 parameters. These four are AppID, UserKey, uuid, and Platform information. UserKey is a unique string generated by DeepBrain AI and should never be disclosed. If you call the API using this authentication key, you will receive available Default AI data and a token to be used in the future. If token refresh is required because the token expired, it can be refreshed by calling AuthStart() again.","title":"Step 2. Prepare authentication-related information"},{"location":"AIPlayerDescription/#step-3-implement-authstart-and-get-the-ai-list","text":"If you have all the necessary AuthStart function parameters mentioned in step 2, you are ready for authentication. Input these parameters inti AuthStart AIHuam.AIAPI.Instance and implement a callback function. If authentication is successful, AI list will be returned. If you do not have permission to any of the AIs, aiList returns null. AIHuman.Core.AIAPI.Instance.AuthStart(\"appId\", \"userKey\", \"UUID\", \"wnds\" , (aiLIst, error) => { string message = string.Empty; if(string.IsNullOrEmpty(error) && aiLIst != null) { string jsonStr = aiLIst.Root.ToString(); AIHuman.Core.AIAPI.AIList list = Newtonsoft.Json.JsonConvert.DeserializeObject<AIHuman.Core.AIAPI.AIList>(jsonStr); message = string.Format(\"Auth Complete, Avaliable Count : {0}\", list.ai.Length); /* e.g.) \"succeed\":true, \"ai\":[{\"aiName\":\"vida\",\"aiDisplayName\":\"Vida\",\"language\":\"en\"}, {\"aiName\":\"bret\",\"aiDisplayName\":\"Bret\",\"language\":\"en\"}, {\"aiName\":\"danny\",\"aiDisplayName\":\"Danny\",\"language\":\"en\"}, {\"aiName\":\"kang\",\"aiDisplayName\":\"Kang\",\"language\":\"ko\"}] */ } else { message = string.Format(\"Auth Error : {0}\", error); } AIHuman.Util.Log.LogWrite(message); } );","title":"Step 3. Implement AuthStart and Get the AI list"},{"location":"AIPlayerDescription/#step-4-initialize-aiplayer-to-the-desired-ai","text":"After authenticating and checking the list of available AIs, it is necessary to initialize the AI first in order to actually use a specific AI. To initialize it, create AIPlayer with the desired AIName as shown below. If there is an existing AIPlayer, delete it and create a new one. Once you initialize, AIPlayer downloads necessary resources based on initial settings and becomes active. You can also receive AI status through callback function registered in the second argument of the AIPlayer constructor. ... public AIPlayer AIPlayerObject // View Binding object { ... } ... private void UpdateSelectedAI() { if (_aiPlayer != null) { _aiPlayer.Dispose(); _aiPlayer = null; } if (_speechList != null) { _speechList.Clear(); _speechList = null; } AIPlayer _aiPlayer = new(SelectedAI.AIName, this); AIPlayerObject = _aiPlayer.GetObject(); SpeechList = new ObservableCollection<string>(AIAPI.Instance.GetSampleTexts(SelectedAI.AIName)); SpeechList.Insert(0, Resource.DefaultSpeech); ... }","title":"Step 4. Initialize AIPlayer to the desired AI"},{"location":"AIPlayerDescription/#2-aiplayer-resources-and-states","text":"Start loading resources When AIPlayer is created after authentication is completed, resource loading starts according to the input AIName , and the resource loading status is reported to the listener (IAIPlayerCallback) registered in the constructor. (Initially, it may take a few minutes for the resource to complete loading.) Monitoring player state through IAIPlayerCallback implementation The values for the parameter AIState.state in the listener method onAIStateChanged(AIStatePublisher.AIState state) are shown below. You can also implement loading progress with onAIPlayerResLoadingProgressed(int current, int total). AIState.RES_LOAD_STARTED : resource loading is started. AIState.RES_LOAD_COMPLETED : resource loading is completed. If there is any problem during this process, the onAIPlayerError() method is called. Typically, a response from the onAIPlayerError() may be notifying the expiration of the authentication token. An appropriate response is required depending on the situation. AIError.SDK_API_ERR : Notifies error in authentication process API. ##### e.g.) 1402 error (value token expired): Token refresh required -> Call AuthStart() method again // AI resource related status CallBack public void onAIStateChanged(AIState state) { ... if (state.state == AIState.RES_LOAD_STARTED) { message = \"AI Resource loading started.\"; ... } else if (state.state == AIState.RES_LOAD_COMPLETED) { message = \"AI Resource loading completed.\"; ... } ... } // AI resource loading progress CallBack public void onAIPlayerResLoadingProgressed(int current, int total) { float progress = ((float) current / (float) total) * 100; message = string.Format(\"AI Resource Loading... {0}%\", (int)progress); } // AI error CallBack public void onAIPlayerError(AIError error) { if (error.SDK_API_ERR == error.errorType) { Debug.LogError(string.Format(\"sdk_ai_Info error : {0}\", error.exInfo)); string errorDesc = error.exInfo; if (string.IsNullOrEmpty(errorDesc)) { JSONObject json = null; try { json = new JSONObject(errorDesc); } catch (JsonException ex) { Log.log(ex.Message); Debug.LogError(string.Format(\"Json Exception Error : {0}\", ex.Message)); } if (json != null && json.optInt(Constants.KEY_ERRORCODE, -1) == Constants.API_ERRORCODE_TOKEN_EXPIRED) { // refresh token } } } }","title":"2. AIPlayer Resources and States"},{"location":"AIPlayerDescription/#3-aiplayer-basic-speaking-features","text":"","title":"3. AIPlayer Basic Speaking Features"},{"location":"AIPlayerDescription/#basic-speaking-using-aiclipset-and-monitor-ai-speaking","text":"After AIPlayer resource is loaded, call Send method . To activate the function, in the sample below, select the sentence to speak through the drop-down menu and click the Play button on the right. In general, speech can be performed using pure text, but speech can also be performed using AIHuman.Common.Model.AIClipSet . Also, speech can be performed along with a specific gesture. For example, you could instruct the AI to say hello by waving his hand. This is called gesture speech. Details are explained in Gesture speech related parts . If the text to speak is too long, it may not be possible to synthesize the resources required for the utterance. There are some models that can synthesize long sentences. Although it varies from ai to ai, it is generally recommended that sentences be cut to an appropriate length in Korean, usually within 30 to 40 characters, and at a similar level in English. // using pure-text _aiPlayer.Send(new[] {\"this is sample sentence.\"}); // using AIClipSet AIClipSet clip = AIAPI.CreateClipSet(\"this is sample sentence.\"); _aiPlayer.Send(new[] {clip}); Speaking related Monitoring After the Send method is called, you can listen to the operation status feedback in the registered listener. This feedback is returned by calling the method (onAIStateChanged) of the listener(IAIPlayerCallback). onAIStateChanged sequentially returns the following AIState values. SPEAKING_PREPARE_STARTED SPEAKING_PREPARE_COMPLETED SPEAKING_STARTED SPEAKING_COMPLETED // Speaking related CallBack example public void onAIStateChanged(AIState state) { if (state.state == AIState.SPEAKING_PREPARE_STARTED) { _txtStatus.text = \"AI started preparation to speak.\"; } else if (state.state == AIState.SPEAKING_PREPARE_COMPLETED) { _txtStatus.text = \"AI finished preparation to speak.\"; } else if (state.state == AIState.SPEAKING_STARTED) { _txtStatus.text = \"AI started speaking.\"; } else if (state.state == AIState.SPEAKING_COMPLETED) { _txtStatus.text = \"AI finished speaking.\"; } } // AI error CallBack example public void onAIPlayerError(AIError error) { if (error.errorType == AIError.SOCKET_ERR) { _txtStatus.text = \"Socket Error: \" + error.exInfo; } else if (error.errorType == AIError.RES_LOAD_ERR) { _txtStatus.text = \"Resource Error: \" + error.exInfo); } else if (error.errorType == AIError.SPEAK_SEND_ERR) { _txtStatus.text = \"Speak Error: \" + error.exInfo); } } AIPlayer Speaking related Features The following are actions that can be performed while the AIPlayer is Speaking.","title":"Basic Speaking using AIClipSet and Monitor AI Speaking"},{"location":"AIPlayerDescription/#pause-speaking","text":": Pause speaking. // pause method _aiPlayer.Pause()","title":"Pause speaking"},{"location":"AIPlayerDescription/#resume-speaking","text":": Resume speaking. (resume from pause) // resume method _aiPlayer.Resume()","title":"Resume Speaking"},{"location":"AIPlayerDescription/#stop-speaking","text":": Stop speaking and reset all data. (cannot resume) // stop method _aiPlayer.StopSpeaking()","title":"Stop speaking"},{"location":"AIPlayerDescription/#4-aiplayer-advanced-speaking-features","text":"All functions other than speaking(mostly related to AI settings) of AIPlayer are described below. After the resource load required for AI operation is completed, some settings of AIPlayer can be adjusted. When the resource loading is completed (RES_LOAD_COMPLETED), the state changes such that actual operations can be performed(Idle). On right side of the panel, Voice, Gesture, Speed , etc. can be adjusted as shown below.","title":"4. AIPlayer Advanced Speaking Features"},{"location":"AIPlayerDescription/#change-ai-speech-rate","text":": You can set the speech rate of AI. The possible value range is from 0.5 to 1.5. // set Property _aiPlayer.Speed = value;","title":"Change AI Speech Rate"},{"location":"AIPlayerDescription/#gestures","text":"As briefly mentioned above, speech can also be performed using ClipSet . The ClipSet here refers to one action unit in a series of AI actions. There are three types of ClipSet: general speech that performs only speaking, speech with gesture, and gesture only. The Gesture can be used depending on whether the AI model supports Gestures , and the list of available gestures can be checked using the GetGestures function of AIPlayer. Even a model that does not support gestures can be operated using ClipSet. Clipset types are as follows. Clip Type CLIP_SPEECH: Clip only for speech without gestures CLIP_GESTURE: Gesture only Clip CLIP_SPEECH_GESTURE: Clip for speech with gestures In the sample screenshot below, an AI model named Jonathan is speaking while waving his hand with a \"hi\" gesture. using AIHuman.Common.Model; using AIHuman.Core; using AIHuman.Media; ... private ObservableCollection<AIGesture> _gestures; ... _gestures = _aiPlayer.GetGestures(); ... AIGesture gesture = _gestures[index]; AIClipSet clip = AIAPI.CreateClipSet(\"nice to meet you.\", gesture.Name); _aiPlayer.Send(new[] {clip}); Monitoring callbacks of gesture actions IAIPlayerCallback.OnAIStateChanged(AIState) is called in the same way as the speech actions. The state value of AIState is called as follows to know the state. However, since AIState.GetAIMsg().Clip.Type, GestureName, and SpeechText are known here, it is possible to know whether it is a gesture action or just a speech action. SPEAKING_PREPARE_STARTED SPEAKING_PREPARE_COMPLETED SPEAKING_STARTED SPEAKING_COMPLETED","title":"Gestures"},{"location":"AIPlayerDescription/#change-the-voice-or-language","text":"Some AIs can speak with other voices besides basic voices. It is also possible to speak other language than the basic voice's language if the supported voice's language is different from the basic language of AI. You can check the sample for a list of voices that are currently available to a AI.","title":"Change the Voice or Language"},{"location":"AIPlayerDescription/#set-the-custom-voice-using-aiplayers-method","text":"You can check which voice AI can use by the following method. CustomVoice has properties of id, name, language, and tag. ObservableCollection<CustomVoice> customVoices = _aiPlayer.GetCustomVoices(); If you know the id of the desired voice, you can find the desired voice using the following method. If there is none, return null. CustomVoice myVoice = _aiPlayer.FindCustomVoice(voiceId); Direct change to the desired voice on the aplayer is set as follows, and is set to the default voice when null is entered. Returns true when success. CustomVoice myVoice = _aiPlayer.GetCustomVoices()[2]; _aiPlayer.SetCustomVoice(myVoice);","title":"Set the custom voice using AIPlayer's method"},{"location":"AIPlayerDescription/#set-the-custom-voice-using-aiclipset","text":"In addition to the method of using the setCustomVoice method to set a voice other than the basic voice, AIClipSet can be used to speak the desired voice as follows. CustomVoice myVoice = _aiPlayer.GetCustomVoices()[0]; AIClipSet aiClipSet = AIAPI.CreateClipSet(\"this is sample sentence.\", null, myVoice); _aiPlayer.Send(new[] {aiClipSet});","title":"Set the custom voice using AIClipSet"},{"location":"AIPlayerDescription/#preload","text":"Preload is used when you want to make the AI speak the next sentence without delay by loading sentences in advance. You could think of it as a caching process. Select a sentence and press the Preload button in the sample below to perform the corresponding action. // using pure-text _aiPlayer.Preload(new[] {\"sentence\"}); // using AIClipSet _aiPlayer.Preload(new[] {clip}); Preload related Monitoring AIPlayerCallback.onAIStateChanged(AIState) is called during the preload operation just like the speaking operation. The value of AIState is shown below. SPEAKING_PREPARE_PRELOAD_STARTED SPEAKING_PREPARE_PRELOAD_COMPLETED When the AI has several sentences to speak, it first processes the very first sentence. Once the returned state from onAIStateChanged is SPEAKING_STARTED, which is when the AI starts to speak the first sentence, the next sentence can be preloaded. If you play the next sentence after the state update to SPEAKING_PREPARE_PRELOAD_COMPLETED, there will be minimum delays between sentences. // AI Preload related CallBack public void onAIStateChanged(AIState aiState) { ... if (aiState.state == AIState.SPEAKING_PREPARE_PRELOAD_STARTED) { _txtStatus.text = \"AI started preparation to preload.\"; } else if (aiState.state == AIState.SPEAKING_PREPARE_PRELOAD_COMPLETED) { _txtStatus.text = \"AI finished preparation to preload.\"; } ... }","title":"Preload"},{"location":"AIPlayerDescription/#speak-multiple-sentences-consecutively","text":"You can give AIPlayer several sentences at once and make them speak sequentially. In the sample, multi-speaking is performed by selecting random sentences from sentences in the ComboBox. It can be one sentence or it can be several sentences. Press the Multi Speak button in the app below to perform the operation. // using pure-text _aiPlayer.Send(new[] {\"sentence1\", \"sentence2\"}); // using AIClipSet _aiPlayer.Send(new[] {clip1, clip2}); Multi Speak related Monitoring IAIPlayerCallback.onAIStateChanged(AIState) is called for each sentence. The possible AIState values are shown below. SPEAKING_PREPARE_STARTED SPEAKING_PREPARE_COMPLETED If you send several sentences, it automatically preloads if possible. In this case, you can see that the delay between utterances when the AI speaks is reduced.","title":"Speak Multiple Sentences Consecutively"},{"location":"AIPlayerDescription/#5-functionalities-other-than-ai-speaking-mainly-related-to-ai-settings","text":"After the resource is loaded, some settings of aiPlayer can be changed while the actual operation is on. In the sample project screen below, you can see that Scale, Margins , etc. can be adjusted.","title":"5. Functionalities other than AI Speaking (mainly related to AI settings)"},{"location":"AIPlayerDescription/#change-ai-sizescale","text":": You can set the size(scale) of AI. The possible value range is from 0.5 to 1.5. // set Property _aiPlayer.Scale = value;","title":"Change AI Size(Scale)"},{"location":"AIPlayerDescription/#change-ai-positionmargin","text":": You can change the position(margins) of AI. It can be adjusted based on the X-axis(Horizontal) and the Y-axis(Vertical). AIHuman.Common.Margin _aiMargin; _aiMargin.X = 64; _aiMargin.Y = 8; // set Property _aiPlayer.Margin = _aiMargin;","title":"Change AI Position(Margin)"},{"location":"AIPlayerDescription/#6-error-index","text":"You can receive the error code and its details as a callback(onAIPlayerError) and take appropriate action. When an error occurs, the onAIPlayerError(AIError) callback function is called. AIError, the argument of this function, contains information about the error. AIError.errorType tells what kind of error has occurred, and you can find out the details of the error as JSON String through the getMessage() function. By using this message, you can take action when a specific error occurs. For example, code 1402 may mean Token expired, and in this case, call AuthStart() to refresh the token. Check the full error types here .","title":"6. Error Index"},{"location":"Introduction/","text":"Introduction Basic Concepts of AI Human (Video Conversation or AILive) AI Human(previously called AILive) SDK can display a trained AI model that resembles real human on the screen in realtime. However, it goes beyond just realtime displaying the AI model. It even allows the AI to speak naturally as if in a video call . The most critical component of the SDK is AIPlayer. AIPlayer is a View(UserControl) component where the AI model is displayed in real time and can be freely positioned. The AI within the AIPlayer was created by training the voice and facial expression of a real person . Therefore, it does not have any artificial sounds and is more natural than the previously existing TTS. In addition, it is possible to select a variety of different models through AIPlayer. When an AI model is selected, the selected AI model in an Idle state is displayed on the screen after the loading process (user authentication and resource loading) as shown in the image above. AI's Idle state is the state in which the AI is listening rather than speaking. In this state, the AI model is not static like the picture above, but is designed to resemble natural human motion as closely as possible by showing movements like blinking or nodding. AIPlayer has a simple structure where all these processes are automatically performed with simple settings. The user can command the client (AIPlayer) in the Idle state to start speaking like 'Hi', 'How are you', etc. Upon receiving this command, the AI will naturally begin to speak, and when finished, it will naturally go back to the Idles state. Users can also adjust the size, position, and speech rate of the AI . In addition, pause, resume, and stop functionalities are provided, which can be used to support variety of manipulations on the screen. AI models are available for Korean, English, Japanese, and Chinese , enabling multi-lingual support.","title":"Introduction"},{"location":"Introduction/#introduction","text":"Basic Concepts of AI Human (Video Conversation or AILive) AI Human(previously called AILive) SDK can display a trained AI model that resembles real human on the screen in realtime. However, it goes beyond just realtime displaying the AI model. It even allows the AI to speak naturally as if in a video call . The most critical component of the SDK is AIPlayer. AIPlayer is a View(UserControl) component where the AI model is displayed in real time and can be freely positioned. The AI within the AIPlayer was created by training the voice and facial expression of a real person . Therefore, it does not have any artificial sounds and is more natural than the previously existing TTS. In addition, it is possible to select a variety of different models through AIPlayer. When an AI model is selected, the selected AI model in an Idle state is displayed on the screen after the loading process (user authentication and resource loading) as shown in the image above. AI's Idle state is the state in which the AI is listening rather than speaking. In this state, the AI model is not static like the picture above, but is designed to resemble natural human motion as closely as possible by showing movements like blinking or nodding. AIPlayer has a simple structure where all these processes are automatically performed with simple settings. The user can command the client (AIPlayer) in the Idle state to start speaking like 'Hi', 'How are you', etc. Upon receiving this command, the AI will naturally begin to speak, and when finished, it will naturally go back to the Idles state. Users can also adjust the size, position, and speech rate of the AI . In addition, pause, resume, and stop functionalities are provided, which can be used to support variety of manipulations on the screen. AI models are available for Korean, English, Japanese, and Chinese , enabling multi-lingual support.","title":"Introduction"},{"location":"MainClassAPIs/","text":"Main Class APIs The APIs of the main classes of AI Human SDK are briefly described. AIHuman.Media.AIPlayer Modifier and Type Method / Property Description ctor AIPlayer(IAIPlayerCallback callback) Create AIPlayer with Default AI and register callback for state monitoring. (valid only when authentication is complete) ctor AIPlayer(string aiName, IAIPlayerCallback callback) Creates AIPlayer with an AI model defined in aiName and registers callback for state monitoring. (valid only when authentication is complete) AIHuman.Media.AIPlayerView GetObject() It is the actual Control object to be linked(binding) with the View (Xaml) of the Cutom App. void Pause() Pauses speaking. void Preload(string[] sentences) Prepare the sentences to be spoken in advance. void Preload(AIClipSet[] clips) Prepare the clips to be play in advance. void Dispose() Called when destroying AIPlayer. void Resume() Continue speaking again from when it was paused. void Send(string[] sentences) Let the AI speak. (using pure-text string) void Send(AIClipSet[] clips) Let the AI play. (using AIHuman.Common.Model.AIClipSet) void StopSpeaking() Stop the current conversation. It also deletes the content in the speaking queue. float Speed { get; set; } Get or Set the AI's speech rate. float Scale { get; set; } Get or Set the AI's scale. AIHuman.Common.Margin Margin { get; set; } Get or Set the AI's margin. System.Windows.Controls.MediaState PlayerState { get; } Get the state of AIPlayer string AIName { get; } Get the AI name. Collection<AIGesture> GetGestures() Get a collection of gesture. (available gestures) Collection<CustomVoice> GetCustomVoices() Get a collection of custom voice. (available custom voices) bool SetCustomVoice(CustomVoice cv) Set AI's voice to cv. return true if success. CustomVoice FindCustomVoice(string id) Find CustomVoice by id. return null if there is none. AIHuman.Interface.IAIPlayerCallback Modifier and Type Method and Description void onAIPlayerError(AIError error) Reporting an error occurred in AIPlayer void onAIPlayerResLoadingProgressed(int current, int total) Reporting the resource loading progress of AIPlayer void onAIStateChanged(AIState state) Reporting the changed state of AI AIHuman.Core.AIAPI Modifier and Type Method and Description void AuthStart(string appId, string userKey, string uuid, string platform, Action<JToken, string> onComplete) Attempts to authenticate with the issued userKey. It passes the response to CallBack(onComplete) and if successful, you can get a usable AI model. bool GenerateToken(string appId, string userKey, string uuid, string platform) Attempts to authenticate to the server with this information and returns true if successful. Newtonsoft.Json.Linq.JObject GetAIList() Get a list of available AI models. AIHuman.Common.AIModelData GetDefaultAIData() Get the default AI information. string[] GetSampleTexts(string aiName) Get AI sample sentences using aiName. string[] GetSampleTextList(string languageCode) Get the list of sample texts for the language from server. AIHuman.Common.Model.AIClipSet CreateClipSet(string speechText, string gestureName = \"\", CustomVoice customVoice = null) Create AIClipSet Object. AIHuman.Common.Model.AIClipSet Modifier and Type Method and Description enum ClipType Clipset types are as follows. - CLIP_SPEECH: Clip only for speech without gestures - CLIP_GESTURE: Gesture only Clip - CLIP_SPEECH_GESTURE: Clip for speech with gestures ClipType Type { get; } Get the clip type set in the AIClipSet. string SpeechText { get; } Get the sentence set in the AIClipSet. string GestureName { get; } Get the gesture name set in the AIClipSet. string ClipId { get; } Get the ID set in the AIClipSet. AIHuman.Common.Model.AIGesture Modifier and Type Method and Description string Name { get; } Get the name of the gesture. bool EnableSpeech { get; } Get whether the gesture is speech-enabled. Returns true if gesture with speech is possible. AIClipSet.ClipType GetClipType() Get the recommended clip type when using the gesture.","title":"Main Class APIs"},{"location":"MainClassAPIs/#main-class-apis","text":"The APIs of the main classes of AI Human SDK are briefly described.","title":"Main Class APIs"},{"location":"MainClassAPIs/#aihumanmediaaiplayer","text":"Modifier and Type Method / Property Description ctor AIPlayer(IAIPlayerCallback callback) Create AIPlayer with Default AI and register callback for state monitoring. (valid only when authentication is complete) ctor AIPlayer(string aiName, IAIPlayerCallback callback) Creates AIPlayer with an AI model defined in aiName and registers callback for state monitoring. (valid only when authentication is complete) AIHuman.Media.AIPlayerView GetObject() It is the actual Control object to be linked(binding) with the View (Xaml) of the Cutom App. void Pause() Pauses speaking. void Preload(string[] sentences) Prepare the sentences to be spoken in advance. void Preload(AIClipSet[] clips) Prepare the clips to be play in advance. void Dispose() Called when destroying AIPlayer. void Resume() Continue speaking again from when it was paused. void Send(string[] sentences) Let the AI speak. (using pure-text string) void Send(AIClipSet[] clips) Let the AI play. (using AIHuman.Common.Model.AIClipSet) void StopSpeaking() Stop the current conversation. It also deletes the content in the speaking queue. float Speed { get; set; } Get or Set the AI's speech rate. float Scale { get; set; } Get or Set the AI's scale. AIHuman.Common.Margin Margin { get; set; } Get or Set the AI's margin. System.Windows.Controls.MediaState PlayerState { get; } Get the state of AIPlayer string AIName { get; } Get the AI name. Collection<AIGesture> GetGestures() Get a collection of gesture. (available gestures) Collection<CustomVoice> GetCustomVoices() Get a collection of custom voice. (available custom voices) bool SetCustomVoice(CustomVoice cv) Set AI's voice to cv. return true if success. CustomVoice FindCustomVoice(string id) Find CustomVoice by id. return null if there is none.","title":"AIHuman.Media.AIPlayer"},{"location":"MainClassAPIs/#aihumaninterfaceiaiplayercallback","text":"Modifier and Type Method and Description void onAIPlayerError(AIError error) Reporting an error occurred in AIPlayer void onAIPlayerResLoadingProgressed(int current, int total) Reporting the resource loading progress of AIPlayer void onAIStateChanged(AIState state) Reporting the changed state of AI","title":"AIHuman.Interface.IAIPlayerCallback"},{"location":"MainClassAPIs/#aihumancoreaiapi","text":"Modifier and Type Method and Description void AuthStart(string appId, string userKey, string uuid, string platform, Action<JToken, string> onComplete) Attempts to authenticate with the issued userKey. It passes the response to CallBack(onComplete) and if successful, you can get a usable AI model. bool GenerateToken(string appId, string userKey, string uuid, string platform) Attempts to authenticate to the server with this information and returns true if successful. Newtonsoft.Json.Linq.JObject GetAIList() Get a list of available AI models. AIHuman.Common.AIModelData GetDefaultAIData() Get the default AI information. string[] GetSampleTexts(string aiName) Get AI sample sentences using aiName. string[] GetSampleTextList(string languageCode) Get the list of sample texts for the language from server. AIHuman.Common.Model.AIClipSet CreateClipSet(string speechText, string gestureName = \"\", CustomVoice customVoice = null) Create AIClipSet Object.","title":"AIHuman.Core.AIAPI"},{"location":"MainClassAPIs/#aihumancommonmodelaiclipset","text":"Modifier and Type Method and Description enum ClipType Clipset types are as follows. - CLIP_SPEECH: Clip only for speech without gestures - CLIP_GESTURE: Gesture only Clip - CLIP_SPEECH_GESTURE: Clip for speech with gestures ClipType Type { get; } Get the clip type set in the AIClipSet. string SpeechText { get; } Get the sentence set in the AIClipSet. string GestureName { get; } Get the gesture name set in the AIClipSet. string ClipId { get; } Get the ID set in the AIClipSet.","title":"AIHuman.Common.Model.AIClipSet"},{"location":"MainClassAPIs/#aihumancommonmodelaigesture","text":"Modifier and Type Method and Description string Name { get; } Get the name of the gesture. bool EnableSpeech { get; } Get whether the gesture is speech-enabled. Returns true if gesture with speech is possible. AIClipSet.ClipType GetClipType() Get the recommended clip type when using the gesture.","title":"AIHuman.Common.Model.AIGesture"},{"location":"ProjectSetup/","text":"Project Set up 1. Create a New Project in Visual Studio(2019). Create New Project > WPF Application > Target Framework > .NET 5.0 WpfApp1 is the default when creating a project. 2. Project Setup. Perform the initial setup of the project. 2-1. Download the SDK for Windows from the AI Human SDK website. 2-2. Move the downloaded SDK and related files to the previously created project path. 2-3. In Solution Explorer, right-click Project > Right-click > Add > Create a new folder with a name. \u200b You can configure the libraries referenced in the solution through the created folder > right-click > Add > Existing item. 2-4. Add the downloaded AIHuman SDK library to the solution item. In Solution Explorer, right-click on \"project\" at the top > Add > Project Reference > Reference Manager > Browse > Add AIHumanSDK.dll and Newtonsoft.Json.dll. You will then be able to see that AIHuman SDK is registered in Dependencies > Assembly in the project tree. 3. Add Layout Component(parent layout) to which AIPlayer will be added to MainWindow.xaml. 4. Write the code below in App.xaml.cs, MainWindowViewModel.cs, and MainWindow. App.xaml.cs First, you need to authenticate. The userKey can be issued by registering the appId on the AI Human website. (Refer to step 1 of Quick Start ) using AIHuman.Core; using Newtonsoft.Json; using System.Windows; namespace WpfApp1 { /// <summary> /// Interaction logic for App.xaml /// </summary> public partial class App : Application { public App() { AIAPI.Instance.AuthStart(\"your appId\", \"your userKey\", \"your uuid\", \"wnds\", (aiLIst, error) => { if (string.IsNullOrEmpty(error) && aiLIst != null) { string jsonStr = aiLIst.Root.ToString(); AIAPI.AIList list = JsonConvert.DeserializeObject<AIAPI.AIList>(jsonStr); // $\"Auth Complete, Avaliable Count: {list.ai.Length}\"; } else { MessageBox.Show($\"AuthStart: {error}\"); } } ); } } } MainWindowViewModel.cs using AIHuman.Common; using AIHuman.Common.Base; using AIHuman.Core; using AIHuman.Interface; using AIHuman.Media; using System; using System.Collections.ObjectModel; using System.Windows; using System.Windows.Threading; namespace WpfApp1 { public class MainWindowViewModel : ViewModelBase, IAIPlayerCallback { private AIPlayer _aiPlayer; public AIPlayerView AIPlayerObject { get => _aiPlayer.GetObject(); private set => OnPropertyChanged(nameof(AIPlayerObject)); } private string _status; public string AIStatusText { get => _status; set { _status = value; OnPropertyChanged(nameof(AIStatusText)); } } private string _inputText; public string InputText { get => _inputText; set { _inputText = value; OnPropertyChanged(nameof(InputText)); } } private ObservableCollection<string> _speechList; public ObservableCollection<string> SpeechList { get => _speechList; private set { _speechList = value; OnPropertyChanged(nameof(SpeechList)); } } public RelayCommand SpeakCommand { get; private set; } public MainWindowViewModel() { SpeechList = new ObservableCollection<string>(); _aiPlayer = new AIPlayer(this); AIPlayerObject = _aiPlayer.GetObject(); SpeakCommand = new RelayCommand(Speak_Command); } public void onAIPlayerError(AIError error) { Application.Current.Dispatcher.BeginInvoke(DispatcherPriority.Render, new Action(() => { SpeechList.Add(error.getMessage()); AIStatusText = nameof(AIError); })); } public void onAIPlayerResLoadingProgressed(int current, int total) { Application.Current.Dispatcher.BeginInvoke(DispatcherPriority.Render, new Action(() => { float progress = ((float)current / (float)total) * 100; AIStatusText = string.Format(\"AI Resource Loading... {0}%\", (int)progress); })); } public void onAIStateChanged(AIState state) { switch (state.state) { case AIState.RES_LOAD_COMPLETED: Application.Current.Dispatcher.BeginInvoke(DispatcherPriority.Render, new Action(() => { AIStatusText = \"AI Resource loading completed.\"; })); break; } } private void Speak_Command(object args) { if (string.IsNullOrEmpty(InputText) == false) { _aiPlayer.Send(new[] { InputText }); SpeechList.Add(InputText); InputText = string.Empty; } } } } MainWindow.xaml <Window x:Class=\"WpfApp1.MainWindow\" xmlns=\"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x=\"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:d=\"http://schemas.microsoft.com/expression/blend/2008\" xmlns:mc=\"http://schemas.openxmlformats.org/markup-compatibility/2006\" xmlns:local=\"clr-namespace:WpfApp1\" d:DataContext=\"{d:DesignInstance Type=local:MainWindowViewModel}\" mc:Ignorable=\"d\" Title=\"MainWindow\" Height=\"450\" Width=\"800\"> <Grid> <Grid.ColumnDefinitions> <ColumnDefinition/> <ColumnDefinition/> </Grid.ColumnDefinitions> <ContentControl Margin=\"0\" Grid.Column=\"0\" Content=\"{Binding Path=AIPlayerObject}\" Focusable=\"False\" /> <Grid Margin=\"0\" Grid.Column=\"1\"> <Grid.RowDefinitions> <RowDefinition Height=\"1*\"/> <RowDefinition Height=\"10*\"/> <RowDefinition/> </Grid.RowDefinitions> <Grid Grid.Row=\"0\"> <Grid.Background> <SolidColorBrush Color=\"#00D3D3\"/> </Grid.Background> <Viewbox> <TextBlock FontWeight=\"Bold\"> <Label Content=\"{Binding AIStatusText}\" /> </TextBlock> </Viewbox> </Grid> <Grid Grid.Row=\"1\"> <DockPanel> <ScrollViewer VerticalScrollBarVisibility=\"Auto\" HorizontalScrollBarVisibility=\"Auto\"> <ItemsControl BorderThickness=\"0\" ItemsSource=\"{Binding SpeechList}\" Focusable=\"False\" /> </ScrollViewer> </DockPanel> </Grid> <Grid Grid.Row=\"2\"> <Grid.ColumnDefinitions> <ColumnDefinition Width=\"7*\"/> <ColumnDefinition/> </Grid.ColumnDefinitions> <TextBox Grid.Column=\"0\" MaxLines=\"1\" FontStretch=\"UltraExpanded\" Text=\"{Binding InputText}\"> </TextBox> <Button Grid.Column=\"1\" HorizontalAlignment=\"Stretch\" Command=\"{Binding SpeakCommand}\"> <TextBlock Padding=\"10, 5\" Text=\"Send\" /> </Button> </Grid> </Grid> </Grid> </Window> MainWindow.xaml.cs using System.Windows; namespace WpfApp1 { /// <summary> /// Interaction logic for MainWindow.xaml /// </summary> public partial class MainWindow : Window { public MainWindow() { InitializeComponent(); DataContext = new MainWindowViewModel(); } } } 5. Command the AI to speak. Build Solution > Run > (Loading Resources) > Enter a sentence in the text box at the bottom right > Click the Send button","title":"Project Set up"},{"location":"ProjectSetup/#project-set-up","text":"1. Create a New Project in Visual Studio(2019). Create New Project > WPF Application > Target Framework > .NET 5.0 WpfApp1 is the default when creating a project. 2. Project Setup. Perform the initial setup of the project. 2-1. Download the SDK for Windows from the AI Human SDK website. 2-2. Move the downloaded SDK and related files to the previously created project path. 2-3. In Solution Explorer, right-click Project > Right-click > Add > Create a new folder with a name. \u200b You can configure the libraries referenced in the solution through the created folder > right-click > Add > Existing item. 2-4. Add the downloaded AIHuman SDK library to the solution item. In Solution Explorer, right-click on \"project\" at the top > Add > Project Reference > Reference Manager > Browse > Add AIHumanSDK.dll and Newtonsoft.Json.dll. You will then be able to see that AIHuman SDK is registered in Dependencies > Assembly in the project tree. 3. Add Layout Component(parent layout) to which AIPlayer will be added to MainWindow.xaml. 4. Write the code below in App.xaml.cs, MainWindowViewModel.cs, and MainWindow. App.xaml.cs First, you need to authenticate. The userKey can be issued by registering the appId on the AI Human website. (Refer to step 1 of Quick Start ) using AIHuman.Core; using Newtonsoft.Json; using System.Windows; namespace WpfApp1 { /// <summary> /// Interaction logic for App.xaml /// </summary> public partial class App : Application { public App() { AIAPI.Instance.AuthStart(\"your appId\", \"your userKey\", \"your uuid\", \"wnds\", (aiLIst, error) => { if (string.IsNullOrEmpty(error) && aiLIst != null) { string jsonStr = aiLIst.Root.ToString(); AIAPI.AIList list = JsonConvert.DeserializeObject<AIAPI.AIList>(jsonStr); // $\"Auth Complete, Avaliable Count: {list.ai.Length}\"; } else { MessageBox.Show($\"AuthStart: {error}\"); } } ); } } } MainWindowViewModel.cs using AIHuman.Common; using AIHuman.Common.Base; using AIHuman.Core; using AIHuman.Interface; using AIHuman.Media; using System; using System.Collections.ObjectModel; using System.Windows; using System.Windows.Threading; namespace WpfApp1 { public class MainWindowViewModel : ViewModelBase, IAIPlayerCallback { private AIPlayer _aiPlayer; public AIPlayerView AIPlayerObject { get => _aiPlayer.GetObject(); private set => OnPropertyChanged(nameof(AIPlayerObject)); } private string _status; public string AIStatusText { get => _status; set { _status = value; OnPropertyChanged(nameof(AIStatusText)); } } private string _inputText; public string InputText { get => _inputText; set { _inputText = value; OnPropertyChanged(nameof(InputText)); } } private ObservableCollection<string> _speechList; public ObservableCollection<string> SpeechList { get => _speechList; private set { _speechList = value; OnPropertyChanged(nameof(SpeechList)); } } public RelayCommand SpeakCommand { get; private set; } public MainWindowViewModel() { SpeechList = new ObservableCollection<string>(); _aiPlayer = new AIPlayer(this); AIPlayerObject = _aiPlayer.GetObject(); SpeakCommand = new RelayCommand(Speak_Command); } public void onAIPlayerError(AIError error) { Application.Current.Dispatcher.BeginInvoke(DispatcherPriority.Render, new Action(() => { SpeechList.Add(error.getMessage()); AIStatusText = nameof(AIError); })); } public void onAIPlayerResLoadingProgressed(int current, int total) { Application.Current.Dispatcher.BeginInvoke(DispatcherPriority.Render, new Action(() => { float progress = ((float)current / (float)total) * 100; AIStatusText = string.Format(\"AI Resource Loading... {0}%\", (int)progress); })); } public void onAIStateChanged(AIState state) { switch (state.state) { case AIState.RES_LOAD_COMPLETED: Application.Current.Dispatcher.BeginInvoke(DispatcherPriority.Render, new Action(() => { AIStatusText = \"AI Resource loading completed.\"; })); break; } } private void Speak_Command(object args) { if (string.IsNullOrEmpty(InputText) == false) { _aiPlayer.Send(new[] { InputText }); SpeechList.Add(InputText); InputText = string.Empty; } } } } MainWindow.xaml <Window x:Class=\"WpfApp1.MainWindow\" xmlns=\"http://schemas.microsoft.com/winfx/2006/xaml/presentation\" xmlns:x=\"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:d=\"http://schemas.microsoft.com/expression/blend/2008\" xmlns:mc=\"http://schemas.openxmlformats.org/markup-compatibility/2006\" xmlns:local=\"clr-namespace:WpfApp1\" d:DataContext=\"{d:DesignInstance Type=local:MainWindowViewModel}\" mc:Ignorable=\"d\" Title=\"MainWindow\" Height=\"450\" Width=\"800\"> <Grid> <Grid.ColumnDefinitions> <ColumnDefinition/> <ColumnDefinition/> </Grid.ColumnDefinitions> <ContentControl Margin=\"0\" Grid.Column=\"0\" Content=\"{Binding Path=AIPlayerObject}\" Focusable=\"False\" /> <Grid Margin=\"0\" Grid.Column=\"1\"> <Grid.RowDefinitions> <RowDefinition Height=\"1*\"/> <RowDefinition Height=\"10*\"/> <RowDefinition/> </Grid.RowDefinitions> <Grid Grid.Row=\"0\"> <Grid.Background> <SolidColorBrush Color=\"#00D3D3\"/> </Grid.Background> <Viewbox> <TextBlock FontWeight=\"Bold\"> <Label Content=\"{Binding AIStatusText}\" /> </TextBlock> </Viewbox> </Grid> <Grid Grid.Row=\"1\"> <DockPanel> <ScrollViewer VerticalScrollBarVisibility=\"Auto\" HorizontalScrollBarVisibility=\"Auto\"> <ItemsControl BorderThickness=\"0\" ItemsSource=\"{Binding SpeechList}\" Focusable=\"False\" /> </ScrollViewer> </DockPanel> </Grid> <Grid Grid.Row=\"2\"> <Grid.ColumnDefinitions> <ColumnDefinition Width=\"7*\"/> <ColumnDefinition/> </Grid.ColumnDefinitions> <TextBox Grid.Column=\"0\" MaxLines=\"1\" FontStretch=\"UltraExpanded\" Text=\"{Binding InputText}\"> </TextBox> <Button Grid.Column=\"1\" HorizontalAlignment=\"Stretch\" Command=\"{Binding SpeakCommand}\"> <TextBlock Padding=\"10, 5\" Text=\"Send\" /> </Button> </Grid> </Grid> </Grid> </Window> MainWindow.xaml.cs using System.Windows; namespace WpfApp1 { /// <summary> /// Interaction logic for MainWindow.xaml /// </summary> public partial class MainWindow : Window { public MainWindow() { InitializeComponent(); DataContext = new MainWindowViewModel(); } } } 5. Command the AI to speak. Build Solution > Run > (Loading Resources) > Enter a sentence in the text box at the bottom right > Click the Send button","title":"Project Set up"},{"location":"SamplePorjectDescription/","text":"Sample Project Description The WPF-based sample covered in this document is an example of using the AI Human SDK. Following this guide, you will be able to try out every functionalities in the AI Human SDK one-by-one through implementing the features in AI Human Sample Application. You can open AIHuman_WPF_Sample.sln first and run the following Sample App after build the solution. The SDK authentication is enabled automatically on this page . When the screen appears, the authentication action is automatically called, so there is no need to take any other action. All you need to do is input the appId, userkey, uuid, and target platform obtained above. Once authenticated, authentication is maintained until the app is closed, so there is no need to authenticate again. If the menu still doesn't work, check if there is an error in the authentication function(AuthStart) callback. Most of the time, token refresh is the problem. Menu on HomeView(HomeView.xaml) Each menu is as follows. Quick Start: Quickly see AI Human in action (QuickStartView.xaml) AI Human Demo: An example of using AI Human SDK (DemoView.xaml) with Playchat & Azure STT: Conversational AI example using AI Human, Playchat, and Azure STT (PlaychatView.xaml) with Google Dialogflow: Conversational AI example using AI Human and Google Dialogflow (DialogFlowView.xaml) Exit: Exit the App (NavigationBar.xaml) 1. Quick Start In Quick Start, the following screen appears when the default AI creation and pre-preparation step is completed through AIPlayer. It may take several minutes until it starts speaking depending on the network conditions, as there is a default loading time on the first run. After the first utterance, you can press the input box at the bottom of the screen to type in a sentence that you want to make the AI say. The default AI is Jonathan. (Basically speaking a language that corresponds to the language value set in AI. However, you can implement multilingual services using the following Change the Voice or Language functionality.) Speak or Enter: Jonathan speaks the sentence entered in the TextBox at the bottom right. Home: Go to HomeView.xaml Exit: Close the app. 2. AI Human Demo AI Human Demo is a page where you can try out various functionalities of AIPlayer. You can try changing to another approved AI model through [AI Select]. For other details, please refer to AIPlayer Description . First, get a list of available AIs and set up the UI. The Constants.appid, userKey, uuid, and targetPlatform below are parameters entered when calling AuthStart in HomeView. AIAPI.Instance.AuthStart(Constants.AppId, Constants.UserKey, Constants.Uuid, Constants.TargetPlatform, (aiList, error) => { // You can get a list of available AIs via CallBack. AIAPI.AIList apiAIlist = JsonConvert.DeserializeObject<AIAPI.AIList>(aiList.Root.ToString()); if (aiList == null) { AIStatusText = Resource.ApiAiListEmptyError; } AIs = new ObservableCollection<AIAPI.AI>(); foreach (AIAPI.AI item in apiAIlist.ai) { AIs.Add(item); } SelectedAI = AIs[0]; }); The part that changes AI. In addition to creating and adding AIPlayer, sample text is obtained and the utterance sentence ComboBox is filled. It is updated by getting the rest of the default settings. private void UpdateSelectedAI() { if (_aiPlayer != null) { _aiPlayer.Dispose(); _aiPlayer = null; } if (_speechList != null) { _speechList.Clear(); _speechList = null; } _aiPlayer = new(SelectedAI.AIName, this); AIPlayerObject = _aiPlayer.GetObject(); SpeechList = new ObservableCollection<string>(AIAPI.Instance.GetSampleTexts(SelectedAI.AIName)); SpeechList.Insert(0, Resource.DefaultSpeech); SpeechIndex = 0; ... } Examples of Speak, Preload, Pause, Multi Speak(Randomly), Resume, and Pause. AIHuman.Core.RelayCommand is used for View and Command Binding. This implementation is only an example and it is not necessary to use AIHuman.Core.RelayCommand. Please refer to the AIPlayer description that follows below. private void Speak_Command(object args) { _sendingMessage.Clear(); _sendingMessage.Add(_speechText); _aiPlayer.Send(_sendingMessage.ToArray()); } private void Preload_Command(object args) { _sendingMessage.Clear(); _sendingMessage.Add(_speechText); _isPreload = true; _aiPlayer.Preload(_sendingMessage.ToArray()); } private void Stop_Command(object args) { _aiPlayer.StopSpeaking(); AIStatusText = Resource.StopStatus; } private void Multi_Command(object args) { Random rand = new(); _sendingMessage.Clear(); for (int i = 1; i < SpeechList.Count; ++i) { if (rand.Next(0, 100) % 6 % 2 == 0) { _sendingMessage.Add(SpeechList[i]); } } _aiPlayer.Send(_sendingMessage.ToArray()); } private void Resume_Command(object args) { _aiPlayer.Resume(); AIStatusText = Resource.ResumeStatus; } private void Pause_Command(object args) { _aiPlayer.Pause(); AIStatusText = Resource.PauseStatus; } By implementing IAIPlayerCallback, it is possible to receive callback from AI operations. public interface IAIPlayerCallback { void onAIPlayerError(AIError error); void onAIPlayerResLoadingProgressed(int current, int total); void onAIStateChanged(AIState state); } Through onAIStateChanged implementation, you can receive CallBack of AI states shown below. SPEAKING_STARTED: AI started speaking. SPEAKING_COMPLETED: AI finished speaking. SPEAKING_PREPARE_STARTED: AI started preparation to speak. RES_LOAD_COMPLETED: AI Resource loading completed. RES_LOAD_STARTED: AI Resource loading started. SPEAKING_PREPARE_COMPLETED: AI finished preparation to speak. 3. with Playchat & Azure STT This demo is an example of a conversational AI service linked with AI Human + Playchat + Azure STT. Basically, AI Human and Playchat are in the form of chatting by the user inputting with the keyboard. Additionally, use Azure STT to speak like a real person . When the AI load is complete, the AI greets you. (\"Hello, long time no see.\") After the greeting, chat or click the STT button at the bottom to get a voice input signal, say \" where are you \". (Actual operation is possible after the Azure STT setup is completed. It is explained below in this chapter.) The AI understands the voice and the AI gives an appropriate answer. Currently, as it is a test chatbot, it can answer only a few limited questions, but if the chatbot is advanced, it can be used in various ways, such as ordering at a restaurant or making a reservation for a performance depending on the situation. In addition, the chatbot can also display images by sending additional information in addition to text. Using AI + Chatbot + Speech Recognition together If you want to use the conversational AI service mentioned in the demo, you need to prepare as follows. Prepare your Playchat Bot ID: Already prepared in the sample (https://www.playchat.ai/docs/en/menual-chatbot-en.html) Prepare Azure Speech Service Key and Endpoint: https://docs.microsoft.com/en-us/azure/cognitive-services/speech-service/overview Assign values to the PLAYCHAT_BOT_ID, AZURE_STT_URL, and AZURE_SUBSCRIPTION_KEY variables declared at the top of the class definition of the PlaychatViewModel.cs file. The main thing is to continue the conversation with AI and voice. For this, AIPlayer, chatbot, and voice recognition must work harmoniously. The class that implements this is PlaychatViewModel . First, after the chatbot is loaded, it sends a \"start\"(Constants.KEY_START) signal to the chatbot, and when the chatbot recognizes it, it sends a greeting message . The greeting is delivered via IChatbotCallback's OnChatbotMessage function. PlaychatViewModel extracts the sentences to be delivered to AI from this message and delivers them to AIPlayer, allowing AI to speak. public void OnChattingReady() { SendMsgToChatbotAndUpdateChatUI(Constants.KEY_START, null); } public void OnChatbotMessage(JObject response) { OnNewChatMessage(response); if (response != null) { // get func_name, args string fName = response.GetValue(Constants.KEY_FUNC_NAME).ToString(); JObject args = response.GetValue(Constants.KEY_ARGS).ToObject<JObject>(); // process for functions accordingly ProcessOnNewFunc(fName, args); } } Data transfer format of Chatbot (Playchat) When speech is recognized through Azure STT, the content is delivered directly to the Playchat server. However, there are situations where you need to manually send a message or special signal to the chatbot. (This includes the \"start\" signal mentioned above.) To send the user's message, use the chatbot's Send function. bool Send(string command, JObject detail) Write the server function name you want in the command, and put the necessary arguments as key:value in detail. The currently set command (or func_name, the same) is as follows. namespace AIHuman.Common.Constants { // send public const string VALUE_USERINPUT_FUNCNAME = \"userInput\"; public const string KEY_START = \"start\"; // recv public const string VALUE_FUNCNAME_ONMESSAGE = \"onMessage\"; // extra public const string VALUE_USERINPUT_NEXT = \":next\"; } In the userInput function, input the argument value with text as the key. start function takes no arguments. OnMessage basically comes with the following values. In particular, if the next value is true for 'extra' here, it means that there are additional messages. /* example {\"func_name\":\"onMessage\", \"args\": {\"kind\":\"Content\", \"text\":\"Hello, long time no see.\", \"languages\":{\"en\":\"Hello, long time no see.\"}, \"image\":{\"url\":\"http://...,\"displayname\":\"office image.jpg\"} \"extra\":{next:true}}} */ If there is an additional message, you can receive the additional message by sending it by adding \":next\" (VALUE_USERINPUT_NEXT) , which is a special argument indication, as the argument text in userInput. private void RequestNextMessageIfNeeded(JObject args) { if (args != null) { JObject extra = (JObject)args.GetValue(Constants.KEY_EXTRA); if (extra != null) { bool next = (bool)extra.GetValue(Constants.KEY_NEXT); if (next) { SendUserInputToChatbot(Constants.VALUE_USERINPUT_NEXT); } } } } Many of the above explanations have been omitted. Open the solution file of the sample and refer to the Playchat.xaml and PlaychatViewModel.cs files. 4. with Google Dialogflow This demo is a conversational AI service using AI Human and Google Dialogflow, and it is another example that shows that AI Human can be used together with other services such as Chatbot. (Actual operation is possible after Google Dialogflow is set up. It is explained below in this chapter.) Using AI + Google Dialogflow together If you want to use the conversational AI service mentioned in the demo, you need to prepare as follows. Preparing the Dialogflow Credential File: https://cloud.google.com/dotnet/docs/reference/Google.Cloud.Dialogflow.V2/latest https://cloud.google.com/docs/authentication/getting-started As mentioned in the above official site, the path of the Credential Json file must be registered in the environment variable. Assign the Credential Json file > project_id value to the DF_PROJECT_ID variable declared at the top of the class definition of the DialogFlowViewModel.cs file. You can create a Chatbot client using Google.Cloud.Dialogflow.V2.SessionsClient.Create(). Create a session through SessionName.FromProjectSession(DF_PROJECT_ID, DF_SESSION_ID) and use it when calling client.DetectIntent or client.DetectIntentAsync. For details, open the solution file of the sample and refer to the DialogFlowView.xaml and DialogFlowViewModel.cs files.","title":"Sample Porject Description"},{"location":"SamplePorjectDescription/#sample-project-description","text":"The WPF-based sample covered in this document is an example of using the AI Human SDK. Following this guide, you will be able to try out every functionalities in the AI Human SDK one-by-one through implementing the features in AI Human Sample Application. You can open AIHuman_WPF_Sample.sln first and run the following Sample App after build the solution. The SDK authentication is enabled automatically on this page . When the screen appears, the authentication action is automatically called, so there is no need to take any other action. All you need to do is input the appId, userkey, uuid, and target platform obtained above. Once authenticated, authentication is maintained until the app is closed, so there is no need to authenticate again. If the menu still doesn't work, check if there is an error in the authentication function(AuthStart) callback. Most of the time, token refresh is the problem. Menu on HomeView(HomeView.xaml) Each menu is as follows. Quick Start: Quickly see AI Human in action (QuickStartView.xaml) AI Human Demo: An example of using AI Human SDK (DemoView.xaml) with Playchat & Azure STT: Conversational AI example using AI Human, Playchat, and Azure STT (PlaychatView.xaml) with Google Dialogflow: Conversational AI example using AI Human and Google Dialogflow (DialogFlowView.xaml) Exit: Exit the App (NavigationBar.xaml)","title":"Sample Project Description"},{"location":"SamplePorjectDescription/#1-quick-start","text":"In Quick Start, the following screen appears when the default AI creation and pre-preparation step is completed through AIPlayer. It may take several minutes until it starts speaking depending on the network conditions, as there is a default loading time on the first run. After the first utterance, you can press the input box at the bottom of the screen to type in a sentence that you want to make the AI say. The default AI is Jonathan. (Basically speaking a language that corresponds to the language value set in AI. However, you can implement multilingual services using the following Change the Voice or Language functionality.) Speak or Enter: Jonathan speaks the sentence entered in the TextBox at the bottom right. Home: Go to HomeView.xaml Exit: Close the app.","title":"1. Quick Start"},{"location":"SamplePorjectDescription/#2-ai-human-demo","text":"AI Human Demo is a page where you can try out various functionalities of AIPlayer. You can try changing to another approved AI model through [AI Select]. For other details, please refer to AIPlayer Description . First, get a list of available AIs and set up the UI. The Constants.appid, userKey, uuid, and targetPlatform below are parameters entered when calling AuthStart in HomeView. AIAPI.Instance.AuthStart(Constants.AppId, Constants.UserKey, Constants.Uuid, Constants.TargetPlatform, (aiList, error) => { // You can get a list of available AIs via CallBack. AIAPI.AIList apiAIlist = JsonConvert.DeserializeObject<AIAPI.AIList>(aiList.Root.ToString()); if (aiList == null) { AIStatusText = Resource.ApiAiListEmptyError; } AIs = new ObservableCollection<AIAPI.AI>(); foreach (AIAPI.AI item in apiAIlist.ai) { AIs.Add(item); } SelectedAI = AIs[0]; }); The part that changes AI. In addition to creating and adding AIPlayer, sample text is obtained and the utterance sentence ComboBox is filled. It is updated by getting the rest of the default settings. private void UpdateSelectedAI() { if (_aiPlayer != null) { _aiPlayer.Dispose(); _aiPlayer = null; } if (_speechList != null) { _speechList.Clear(); _speechList = null; } _aiPlayer = new(SelectedAI.AIName, this); AIPlayerObject = _aiPlayer.GetObject(); SpeechList = new ObservableCollection<string>(AIAPI.Instance.GetSampleTexts(SelectedAI.AIName)); SpeechList.Insert(0, Resource.DefaultSpeech); SpeechIndex = 0; ... } Examples of Speak, Preload, Pause, Multi Speak(Randomly), Resume, and Pause. AIHuman.Core.RelayCommand is used for View and Command Binding. This implementation is only an example and it is not necessary to use AIHuman.Core.RelayCommand. Please refer to the AIPlayer description that follows below. private void Speak_Command(object args) { _sendingMessage.Clear(); _sendingMessage.Add(_speechText); _aiPlayer.Send(_sendingMessage.ToArray()); } private void Preload_Command(object args) { _sendingMessage.Clear(); _sendingMessage.Add(_speechText); _isPreload = true; _aiPlayer.Preload(_sendingMessage.ToArray()); } private void Stop_Command(object args) { _aiPlayer.StopSpeaking(); AIStatusText = Resource.StopStatus; } private void Multi_Command(object args) { Random rand = new(); _sendingMessage.Clear(); for (int i = 1; i < SpeechList.Count; ++i) { if (rand.Next(0, 100) % 6 % 2 == 0) { _sendingMessage.Add(SpeechList[i]); } } _aiPlayer.Send(_sendingMessage.ToArray()); } private void Resume_Command(object args) { _aiPlayer.Resume(); AIStatusText = Resource.ResumeStatus; } private void Pause_Command(object args) { _aiPlayer.Pause(); AIStatusText = Resource.PauseStatus; } By implementing IAIPlayerCallback, it is possible to receive callback from AI operations. public interface IAIPlayerCallback { void onAIPlayerError(AIError error); void onAIPlayerResLoadingProgressed(int current, int total); void onAIStateChanged(AIState state); } Through onAIStateChanged implementation, you can receive CallBack of AI states shown below. SPEAKING_STARTED: AI started speaking. SPEAKING_COMPLETED: AI finished speaking. SPEAKING_PREPARE_STARTED: AI started preparation to speak. RES_LOAD_COMPLETED: AI Resource loading completed. RES_LOAD_STARTED: AI Resource loading started. SPEAKING_PREPARE_COMPLETED: AI finished preparation to speak.","title":"2. AI Human Demo"},{"location":"SamplePorjectDescription/#3-with-playchat-azure-stt","text":"This demo is an example of a conversational AI service linked with AI Human + Playchat + Azure STT. Basically, AI Human and Playchat are in the form of chatting by the user inputting with the keyboard. Additionally, use Azure STT to speak like a real person . When the AI load is complete, the AI greets you. (\"Hello, long time no see.\") After the greeting, chat or click the STT button at the bottom to get a voice input signal, say \" where are you \". (Actual operation is possible after the Azure STT setup is completed. It is explained below in this chapter.) The AI understands the voice and the AI gives an appropriate answer. Currently, as it is a test chatbot, it can answer only a few limited questions, but if the chatbot is advanced, it can be used in various ways, such as ordering at a restaurant or making a reservation for a performance depending on the situation. In addition, the chatbot can also display images by sending additional information in addition to text. Using AI + Chatbot + Speech Recognition together If you want to use the conversational AI service mentioned in the demo, you need to prepare as follows. Prepare your Playchat Bot ID: Already prepared in the sample (https://www.playchat.ai/docs/en/menual-chatbot-en.html) Prepare Azure Speech Service Key and Endpoint: https://docs.microsoft.com/en-us/azure/cognitive-services/speech-service/overview Assign values to the PLAYCHAT_BOT_ID, AZURE_STT_URL, and AZURE_SUBSCRIPTION_KEY variables declared at the top of the class definition of the PlaychatViewModel.cs file. The main thing is to continue the conversation with AI and voice. For this, AIPlayer, chatbot, and voice recognition must work harmoniously. The class that implements this is PlaychatViewModel . First, after the chatbot is loaded, it sends a \"start\"(Constants.KEY_START) signal to the chatbot, and when the chatbot recognizes it, it sends a greeting message . The greeting is delivered via IChatbotCallback's OnChatbotMessage function. PlaychatViewModel extracts the sentences to be delivered to AI from this message and delivers them to AIPlayer, allowing AI to speak. public void OnChattingReady() { SendMsgToChatbotAndUpdateChatUI(Constants.KEY_START, null); } public void OnChatbotMessage(JObject response) { OnNewChatMessage(response); if (response != null) { // get func_name, args string fName = response.GetValue(Constants.KEY_FUNC_NAME).ToString(); JObject args = response.GetValue(Constants.KEY_ARGS).ToObject<JObject>(); // process for functions accordingly ProcessOnNewFunc(fName, args); } } Data transfer format of Chatbot (Playchat) When speech is recognized through Azure STT, the content is delivered directly to the Playchat server. However, there are situations where you need to manually send a message or special signal to the chatbot. (This includes the \"start\" signal mentioned above.) To send the user's message, use the chatbot's Send function. bool Send(string command, JObject detail) Write the server function name you want in the command, and put the necessary arguments as key:value in detail. The currently set command (or func_name, the same) is as follows. namespace AIHuman.Common.Constants { // send public const string VALUE_USERINPUT_FUNCNAME = \"userInput\"; public const string KEY_START = \"start\"; // recv public const string VALUE_FUNCNAME_ONMESSAGE = \"onMessage\"; // extra public const string VALUE_USERINPUT_NEXT = \":next\"; } In the userInput function, input the argument value with text as the key. start function takes no arguments. OnMessage basically comes with the following values. In particular, if the next value is true for 'extra' here, it means that there are additional messages. /* example {\"func_name\":\"onMessage\", \"args\": {\"kind\":\"Content\", \"text\":\"Hello, long time no see.\", \"languages\":{\"en\":\"Hello, long time no see.\"}, \"image\":{\"url\":\"http://...,\"displayname\":\"office image.jpg\"} \"extra\":{next:true}}} */ If there is an additional message, you can receive the additional message by sending it by adding \":next\" (VALUE_USERINPUT_NEXT) , which is a special argument indication, as the argument text in userInput. private void RequestNextMessageIfNeeded(JObject args) { if (args != null) { JObject extra = (JObject)args.GetValue(Constants.KEY_EXTRA); if (extra != null) { bool next = (bool)extra.GetValue(Constants.KEY_NEXT); if (next) { SendUserInputToChatbot(Constants.VALUE_USERINPUT_NEXT); } } } } Many of the above explanations have been omitted. Open the solution file of the sample and refer to the Playchat.xaml and PlaychatViewModel.cs files.","title":"3. with Playchat &amp; Azure STT"},{"location":"SamplePorjectDescription/#4-with-google-dialogflow","text":"This demo is a conversational AI service using AI Human and Google Dialogflow, and it is another example that shows that AI Human can be used together with other services such as Chatbot. (Actual operation is possible after Google Dialogflow is set up. It is explained below in this chapter.) Using AI + Google Dialogflow together If you want to use the conversational AI service mentioned in the demo, you need to prepare as follows. Preparing the Dialogflow Credential File: https://cloud.google.com/dotnet/docs/reference/Google.Cloud.Dialogflow.V2/latest https://cloud.google.com/docs/authentication/getting-started As mentioned in the above official site, the path of the Credential Json file must be registered in the environment variable. Assign the Credential Json file > project_id value to the DF_PROJECT_ID variable declared at the top of the class definition of the DialogFlowViewModel.cs file. You can create a Chatbot client using Google.Cloud.Dialogflow.V2.SessionsClient.Create(). Create a session through SessionName.FromProjectSession(DF_PROJECT_ID, DF_SESSION_ID) and use it when calling client.DetectIntent or client.DetectIntentAsync. For details, open the solution file of the sample and refer to the DialogFlowView.xaml and DialogFlowViewModel.cs files.","title":"4. with Google Dialogflow"}]}